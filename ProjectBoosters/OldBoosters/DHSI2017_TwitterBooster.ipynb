{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIers\n",
    "You get the twitter content as a learning boost (below). Once you can play with this a bit then you can see about applying similar principles to your specific APIs.\n",
    "\n",
    "### Setting up credentials\n",
    "The following will need to be run new every time you want to start doing some scraping.  Tweepy will need to be installed and you'll need to go to the command line for this: `$ pip install tweepy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'Your key here'\n",
    "consumer_secret = 'Your secret here'\n",
    "access_token = 'Your key here'\n",
    "access_secret = 'Your secret here'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping 100 Tweets\n",
    "This next bit of code will scrape the last 100 tweets from twitter for a given search term (in this case \"#trump\").  With the credentials set up and the tweepy library all the magic happens on just one line.  The rest is just seeing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = api.search(\"#trump\", count=100) #100 is the max that the api search will return\n",
    "print(len(results))\n",
    "for item in results:\n",
    "    print(item.text)\n",
    "    print(type(item))\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the 100 tweets\n",
    "If you want to save the results to a file then you should run something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"trumpTweets4.json\", \"w\") as f:\n",
    "    print(len(results))\n",
    "    for tweet in results:\n",
    "        f.write(json.dumps(tweet._json) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the tweets\n",
    "We're going to break up each tweet into its respective \"words\" and count them.  You will likely get an error the first time you run the NLTK.  _READ_ the error.  It is unlikely that you will have success implementing the solution in Jupyter Notebooks so I suggest you try doing it with python on the command line instead.  I'll give you a hint: \n",
    "\n",
    "    $ python\n",
    "    <bunch of stuff about Python>\n",
    "    >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "print(len(response))\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'RT', 'â€¦']\n",
    "\n",
    "count_all = Counter()\n",
    "for tweet in response:\n",
    "    text=tweet.text\n",
    "    #print(text)\n",
    "    terms = [term.lower() for term in TweetTokenizer().tokenize(text) if term not in stop]\n",
    "    count_all.update(terms)\n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Sample\n",
    "This code is completely self-contained.  It is here so that you can play with it and get a feeling for how the `matplotlib` library works.  Change the values.  Comment out lines.  Play with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#special ipython/jupyter command that keeps the output in this window rather than opening another one.\n",
    "%matplotlib inline \n",
    "\n",
    "plt.figure()\n",
    "plt.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3)\n",
    "plt.scatter([0.3, 3.8, 1.2, 2.5], [11, 25, 9, 26], color='darkgreen', marker='^')\n",
    "plt.bar([1,2,3,4],[12,3,25,18], width=0.2, align='center')\n",
    "plt.xlim(0.5, 4.5)\n",
    "plt.ylim(0,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Your Tweet Data\n",
    "Now that you have a feel for `matplotlib` compare what is above to what is below.  Again, make sure that you are testing and poking at the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "count_all_dict = dict(count_all.most_common(10))\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.figure(figsize=(100, 40))\n",
    "plt.bar(range(len(count_all_dict)), count_all_dict.values(), align='center')\n",
    "plt.xticks(range(len(count_all_dict)), list(count_all_dict.keys()),rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from the Twitter stream...\n",
    "What is below will open a twitter stream and pull down every tweet that it can until you either turn it off or you run out of space to store it.  _Be warned_, you can collect a lot of data very quickly (a popular search term will generate hundreds of megabytes of data in a day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/\n",
    "# IMPORTANT: this cell will run until you stop it, it crashes, or you run out of space.\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('trumpStream.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['#trump'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to check that you know what you are doing?  Here's some things to try:\n",
    "\n",
    "0. Add comments to the code that are the equivalent of _you_ explaining to your future self what the code is doing.\n",
    "1. Change the code so that when it is run you are asked what to search for and your response becomes the search term.\n",
    "2. Change the code so that the 100 tweet grabber runs all at once rather than with you needing to run it one cell at a time.\n",
    "3. Play with the sentiment analysis tool in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
